{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"colab":{"provenance":[]}},"cells":[{"cell_type":"code","metadata":{"id":"S-D3dDUbKYwD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1670005191190,"user_tz":420,"elapsed":855,"user":{"displayName":"Jay Newby","userId":"15467205112857765885"}},"outputId":"36c6acf0-c205-4768-b644-ff18d20991fa"},"source":["%pylab inline\n","%config InlineBackend.figure_format = 'retina'\n","from ipywidgets import interact"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Populating the interactive namespace from numpy and matplotlib\n"]}]},{"cell_type":"markdown","metadata":{"id":"JqhCThoCkT25"},"source":["# A simple example of a rudimentary autodifferentiation package\n","I used some advanced features in this notebook: classes, inheritance, and generators. I tried to comment and include some additional examples (see Extra Notes at the end)\n","\n","**Warning: this is just an experimntal example. In practice, we will use TensorFlow for tasks that involve automatic differentiation.**"]},{"cell_type":"code","metadata":{"id":"oiblaVN9KYwG","executionInfo":{"status":"ok","timestamp":1670005191861,"user_tz":420,"elapsed":1,"user":{"displayName":"Jay Newby","userId":"15467205112857765885"}}},"source":["class Autodiff_Node(object): \n","    ## A class is a recipe for creating objects (with methods and atributes).\n","    ## This is called a 'base class', which is like a boiler plate recipe that \n","    ## many other classes will use a starting point, each making specific \n","    ## changes.\n","\n","\n","    ## All methods (unless otherwise specified) must have the first argument\n","    ## a variable called `self`, which is a copy of the object itself. Hence,\n","    ## one can access any method or atribute in the object throught the `self`\n","    ## variable.\n","    def __init__(self, parents): \n","        \"\"\"Parameters:\n","        ---------------\n","        `parents` a list of `Autodiff_Node` objects corresponding to the graph\n","            parents.\"\"\"\n","        ## initializer gets called once when you create (or instantiate) an \n","        ## object\n","        self._set_parents(parents)\n","        self._output_data = None\n","    def _set_parents(self, parents):\n","        self.parents = parents\n","        return None\n","    def set_output_data(self, y):\n","        self._output_data = y\n","        return None\n","    def get_output_data(self):\n","        return self._output_data\n","    ## a static modthod just means it doesn't depend on the data in `self`, so \n","    ## `self` does not need to be an argument\n","    @staticmethod \n","    def function(x): \n","        \"\"\"Given input `x` return output `y`\"\"\"\n","        ## this is just a place holder (or template) to be used to create \n","        ## specific types of Node objects\n","        return NotImplementedError\n","    ## a static modthod just means it doesn't depend on the data in `self`, so \n","    ## `self` does not need to be an argument\n","    @staticmethod\n","    def backpropagation_function(x, y, output_gradient): \n","        \"\"\"\n","        Parameters:\n","        --------------------\n","        `x` is the input variable(s): a list of tensors one for each input from \n","            a graph parent.\n","        `y` is the output variable(s): a list of tensors one for each ouput to \n","            a graph child.\n","        `output_gradient` is the gradient (list of partial derivatives) of a \n","            scalar function with respect to one or more output variables.\n","        \n","        Returns:\n","        --------------------\n","        `input_gradient` is the gradient (list of partial derivatives) of a \n","            scalar function with respect to one or more input variables.\"\"\"\n","        ## this is just a place holder (or template) to be used to create \n","        ## specific types of Node objects\n","        return NotImplementedError\n","    def eval(self):\n","        \"\"\"Evaluate the output of the node, moving from necessary inputs \n","        through the DAG in the forward direction.\"\"\"\n","        ## recursively call eval for each node until input variables are reached\n","        x = [node.eval() for node in self.parents]\n","        return self.function(x)\n","    def _eval_and_save_output(self):\n","        ## this is a stateful approach and should be used with care. This method \n","        ## will alter one of the atributes. This can lead to confusing and hard \n","        ## to diagnose bugs. It is best to avoid doing this whenever possible.\n","\n","        ## recursively call eval for each node until inputs are reached\n","        x = [node._eval_and_save_output() for node in self.parents]\n","        y = self.function(x)\n","        ## internal data, or state, is modified here. Specifically the \n","        ## `self._output_data` attribute.\n","        self.set_output_data(y) \n","        return y\n","    def _get_gradient(self, output_gradient):\n","        ## This is a helper function to assemble the gradients, moving backward \n","        ## through the DAG. We must call `_eval_and_save_output()` before \n","        ## using this method\n","        x = [node.get_output_data() for node in self.parents]\n","        ## We use internal state here, which assumes that \n","        ## `_eval_and_save_output()` was called before using this method\n","        y = self.get_output_data() \n","        input_gradient = self.backpropagation_function(x, y, output_gradient)\n","        ## We use recursion combined with generators (see examples at the end of \n","        ## this notebook)\n","        for node, sub_gradient in zip(self.parents, input_gradient):\n","            ## recursive call to the same method attached to the parent nodes\n","            for inner_gradient in node._get_gradient(sub_gradient): \n","                yield inner_gradient\n","    def compute_gradient(self): \n","        \"\"\"Assumes the node has scalar output\"\"\"\n","        ## computing gradients is very simple with the `Autodiff_node` class\n","\n","        ## the dangerous stateful call must precede the gradient calculation\n","        self._eval_and_save_output() \n","        ## the input is always simply `1.0` because partial_L/partial_L = 1\n","        return [g for g in self._get_gradient(1.)] \n","    def __add__(self, b):\n","        ## You can define the \"+\" operator (and other operators)\n","        a = self\n","        return Add(a, b)"],"execution_count":3,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"emklmc5u2Y4N","executionInfo":{"status":"ok","timestamp":1670005192015,"user_tz":420,"elapsed":2,"user":{"displayName":"Jay Newby","userId":"15467205112857765885"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"Kvpj7KgUKtKK","executionInfo":{"status":"ok","timestamp":1670005193350,"user_tz":420,"elapsed":172,"user":{"displayName":"Jay Newby","userId":"15467205112857765885"}}},"source":["class Add(Autodiff_Node):\n","    \"\"\"Add two input nodes\"\"\"\n","    ## this defines a node type specifically for addition, it 'inherits' all \n","    ## of the methods and atributes from its base class, `Autodiff_Node`. Think\n","    ## of these as default methods. Any methods that are redefined here are used \n","    ## instead of the default methods from the base class\n","    def __init__(self, a, b):\n","        ## initializer gets called once when you create (or instantiate) an \n","        ## object\n","        parents = [a, b]\n","        super().__init__(parents) ## calls `__init__` method of the base class\n","    ## a static modthod just means it doesn't depend on the data in `self`, so \n","    ## `self` does not need to be an argument\n","    @staticmethod\n","    def function(x):\n","        a = x[0]\n","        b = x[1]\n","        return a + b\n","    @staticmethod\n","    def backpropagation_function(x, y, output_gradient):\n","        input_gradient = [output_gradient*1, output_gradient*1]\n","        return input_gradient\n","\n","class Multiply(Autodiff_Node):\n","    \"\"\"Multiply two input nodes\"\"\"\n","    def __init__(self, a, b):\n","        parents = [a, b]\n","        super().__init__(parents)\n","    @staticmethod\n","    def function(x):\n","        a = x[0]\n","        b = x[1]\n","        return a*b\n","    @staticmethod\n","    def backpropagation_function(x, y, output_gradient):\n","        a = x[0]\n","        b = x[1]\n","        input_gradient = [output_gradient*b, output_gradient*a]\n","        return input_gradient\n","\n","class Tanh(Autodiff_Node):\n","    \"\"\"Apply the `tanh` function to an input node\"\"\"\n","    def __init__(self, x):\n","        parents = [x]\n","        super().__init__(parents)\n","    @staticmethod\n","    def function(x):\n","        return np.tanh(x[0])\n","    @staticmethod\n","    def backpropagation_function(x, y, output_gradient):\n","        dydx = 1./np.cosh(x[0])**2\n","        input_gradient = [output_gradient*dydx]\n","        return input_gradient\n","\n","class Input_Variable(Autodiff_Node):\n","    \"\"\"Input Variables have a specific fixed value. Use these to hold parameters \n","    and variables. Gradient of a node with a scalar output will be a list of \n","    partial derivatives with respect to these Input Variables.\n","    \n","    Parameters:\n","    ---------------\n","    `value` the numerical value of the variable (scalar in this example).\"\"\"\n","    def __init__(self, value):\n","        self.value = value\n","        parents = []\n","        super().__init__(parents)\n","    @staticmethod\n","    def function(x):\n","        return self.value\n","    @staticmethod\n","    def backpropagation_function(x, y, output_gradient):\n","        input_gradient = output_gradient\n","        return input_gradient\n","    def eval(self): \n","        ## this overrides the default `eval` method defined in `Autodiff_Node`\n","        ## base class\n","        return self.value\n","    def _eval_and_save_output(self): ## another override\n","        self.set_output_data(self.value)\n","        return self.value\n","    def _get_gradient(self, output_gradient): ## another override\n","        yield output_gradient"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TMX-1uAijCTj"},"source":["# Simple example"]},{"cell_type":"code","metadata":{"id":"NVnhB3YMKtFE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1670005254315,"user_tz":420,"elapsed":139,"user":{"displayName":"Jay Newby","userId":"15467205112857765885"}},"outputId":"a86bd717-8360-432a-f26d-952a8e3d38d6"},"source":["w = Input_Variable(1.2)\n","u = Input_Variable(2.)\n","b = Input_Variable(-3.)\n","\n","s1 = Multiply(w, u)\n","s2 = Add(s1, b)\n","# s2 = s1 + b\n","\n","L = Tanh(s2)\n","\n","L.eval()"],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["-0.5370495669980354"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"XvdlyTHJKtH4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1670005254658,"user_tz":420,"elapsed":2,"user":{"displayName":"Jay Newby","userId":"15467205112857765885"}},"outputId":"e658a931-38fa-42d0-98de-048703484caa"},"source":["L.compute_gradient()"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[1.4231555251744454, 0.8538933151046673, 0.7115777625872227]"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"OVFMapxVKs_U","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1670005255346,"user_tz":420,"elapsed":3,"user":{"displayName":"Jay Newby","userId":"15467205112857765885"}},"outputId":"8237d171-3042-4009-fcba-fe77b893c046"},"source":["## exact gradient for comparison\n","_g1 = 1./cosh(w.eval()*u.eval() + b.eval())**2\n","print('gradient (w, u, b):', _g1*u.eval(), _g1*w.eval(), _g1)"],"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["gradient (w, u, b): 1.4231555251744454 0.8538933151046673 0.7115777625872227\n"]}]},{"cell_type":"markdown","metadata":{"id":"PIqa5ysrj6Lv"},"source":["### We can evaluate at any of the nodes"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bthffAGuj5u5","executionInfo":{"status":"ok","timestamp":1670005257558,"user_tz":420,"elapsed":156,"user":{"displayName":"Jay Newby","userId":"15467205112857765885"}},"outputId":"9bc9ef70-4ac5-433a-cd81-f6bd2ac16aa0"},"source":["s2.eval()"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["-0.6000000000000001"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5azfWu5jj5m4","executionInfo":{"status":"ok","timestamp":1670005258888,"user_tz":420,"elapsed":273,"user":{"displayName":"Jay Newby","userId":"15467205112857765885"}},"outputId":"632e8cf5-69f7-493b-e6d0-be56684e81fa"},"source":["s2.compute_gradient()"],"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[2.0, 1.2, 1.0]"]},"metadata":{},"execution_count":11}]},{"cell_type":"markdown","metadata":{"id":"ug3ESbsqh-X2"},"source":["# What happens when an input variable is an input into more than one node?"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"392h9vbSgvW4","executionInfo":{"status":"ok","timestamp":1670005201870,"user_tz":420,"elapsed":161,"user":{"displayName":"Jay Newby","userId":"15467205112857765885"}},"outputId":"3cf59e64-885a-4cc4-f77c-21729f9da845"},"source":["w = Input_Variable(1.2)\n","u = Input_Variable(2.)\n","b = Input_Variable(-10.)\n","\n","s1 = Multiply(w, u)\n","s2 = Add(s1, b)\n","\n","s3 = Multiply(s2, u)\n","\n","L = Tanh(s3)\n","\n","L.eval()"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["-0.9999999999998745"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4H99yhe7iQXa","executionInfo":{"status":"ok","timestamp":1670005204085,"user_tz":420,"elapsed":152,"user":{"displayName":"Jay Newby","userId":"15467205112857765885"}},"outputId":"069bde54-78ce-421d-d4af-6907929aea6c"},"source":["L.compute_gradient() ## four outputs but we only have three input variables"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[1.0036163614810117e-12,\n"," 6.02169816888607e-13,\n"," 5.018081807405058e-13,\n"," -1.906871086813922e-12]"]},"metadata":{},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"pox1TfXAipyn"},"source":["### The resulting gradient has two components for the same variable\n","We probably just need to sum all of the elements of the output that correspond to the same input variable\n","-------------------------------------------------------------\n"]},{"cell_type":"markdown","metadata":{"id":"axHzx2h-k7TF"},"source":["# Generalizations:\n","  1. We could generalize this to account for multidimensional array valued inputs and outputs (this might work in the current form for some operations). For example, suppose we want to do a matrix vector product. We wouldn't want to create a gigantic graph with all the little individual additions and multiplications. It would be far more efficient to define a new node type. We could call it `Matrix_Vector_Product` for example. \n","  2. We could also generalize so that we can use `if` statements, possibly even loops"]},{"cell_type":"markdown","metadata":{"id":"IWIEPPxqiR2W"},"source":["# Extra notes:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cS5ECGPZl6a4","executionInfo":{"status":"ok","timestamp":1666893851317,"user_tz":360,"elapsed":227,"user":{"displayName":"Jay Newby","userId":"15467205112857765885"}},"outputId":"ead278c0-f0ef-419e-99b5-8fdad02764c3"},"source":["## a little generator example \n","[1 for j in arange(4)]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[1, 1, 1, 1]"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Jp7F-9dImAWt","executionInfo":{"status":"ok","timestamp":1666893851483,"user_tz":360,"elapsed":4,"user":{"displayName":"Jay Newby","userId":"15467205112857765885"}},"outputId":"f63a1c78-a5f9-4853-a18b-e13406b26a08"},"source":["## a little generator example \n","[j for j in arange(4)]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[0, 1, 2, 3]"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VbGyJ_YvmAMc","executionInfo":{"status":"ok","timestamp":1666893851972,"user_tz":360,"elapsed":4,"user":{"displayName":"Jay Newby","userId":"15467205112857765885"}},"outputId":"c088becb-5b6f-473c-bf05-36a8d0a0d91b"},"source":["## a little generator example \n","[[i for i in arange(j)] for j in arange(4)]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[[], [0], [0, 1], [0, 1, 2]]"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"88hmdZHG5N9L","executionInfo":{"status":"ok","timestamp":1669833764993,"user_tz":420,"elapsed":379,"user":{"displayName":"Jay Newby","userId":"15467205112857765885"}},"outputId":"3c06daa1-acb6-49f9-d6ac-2fbfc8f63b3c"},"source":["## an advanced generator example using `yield` statements\n","def reverse_arange(n):\n","    for i in arange(n):\n","        yield n - 1 - i\n","[val for val in reverse_arange(3)]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[2, 1, 0]"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","source":[],"metadata":{"id":"KWs1tQx_4Mbh"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pYncH3eS5bFz","executionInfo":{"status":"ok","timestamp":1666898422019,"user_tz":360,"elapsed":166,"user":{"displayName":"Jay Newby","userId":"15467205112857765885"}},"outputId":"d7c3c0db-b59d-4ecd-fb49-99f90b8b9859"},"source":["## an advanced generator example using `yield` statements\n","def i1(n, m):\n","    for i in arange(n):\n","        for j in i2(m):\n","            yield j\n","def i2(n):\n","    for i in arange(n):\n","        yield i\n","\n","[val for val in i1(3, 2)]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[0, 1, 0, 1, 0, 1]"]},"metadata":{},"execution_count":29}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3Wy1wKAl5ogI","executionInfo":{"status":"ok","timestamp":1666898438033,"user_tz":360,"elapsed":458,"user":{"displayName":"Jay Newby","userId":"15467205112857765885"}},"outputId":"4c533237-6896-49d5-cfec-eee01884b8d8"},"source":["[val for val in i1(2, 3)]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[0, 1, 2, 0, 1, 2]"]},"metadata":{},"execution_count":30}]},{"cell_type":"code","metadata":{"id":"a6azXWr1Ks0y","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1666898489177,"user_tz":360,"elapsed":412,"user":{"displayName":"Jay Newby","userId":"15467205112857765885"}},"outputId":"dc65163e-c5b4-4067-e242-7c6f5a4c1d10"},"source":["## an advanced generator example using `yield` statements\n","def i1(n, m):\n","    for i in arange(n):\n","        for j in i2(m):\n","            yield j\n","def i2(n):\n","    for i in arange(n):\n","        for j in i3():\n","            yield j\n","def i3():\n","    yield 5\n","    \n","[val for val in i1(3, 2)]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[5, 5, 5, 5, 5, 5]"]},"metadata":{},"execution_count":31}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2CqQEUCi6Sp-","executionInfo":{"status":"ok","timestamp":1666893853257,"user_tz":360,"elapsed":4,"user":{"displayName":"Jay Newby","userId":"15467205112857765885"}},"outputId":"931aeb53-cf53-4b0f-aee3-4f2c45e98de3"},"source":["## an advanced generator example using `yield` statements\n","def i1(n, m):\n","    for i in arange(n):\n","        for j in i2(m):\n","            yield j\n","def i2(n):\n","    for i in arange(n):\n","        for j in i3():\n","            yield j\n","def i3():\n","    yield 5\n","    yield 3\n","\n","    \n","[val for val in i1(3, 2)]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3]"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","source":["x = 1. + 5j\n","?x"],"metadata":{"id":"wpuAqRVFna__"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def complex_add(x, y):\n","    return (x.real + y.real, x.imag + y.imag)"],"metadata":{"id":"dINgWCjR8Pyb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ohMEtL4H8tON"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Further Reading:\n","You can read more about how automatic differentiation is implemented in the powerful STAN library here https://arxiv.org/abs/1509.07164"],"metadata":{"id":"2f4xYuCz8qhY"}},{"cell_type":"code","source":[],"metadata":{"id":"Co7Sp5ST8s7-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Dual numbers\n","Dual numbers are a bit like complex numbers. Recall, a complex number can be written as \n","$$ z = x + iy,$$\n","where $i$ is the imaginary unit. Using $i^2 = -1$, one can work out the rules for arithmetic operations over complex numbers. For example, \n","$$z_1\\cdot z_2 = (x_1 + iy_1)(x_2 + iy_2) = x_1x_2 - y_1y_2  + i(x_1y_2 + y_1x_2).$$\n","\n","Dual numbers can be written as\n","$$\\xi = y + \\epsilon y',$$\n","where $y\\in\\mathbb{R}$, $y'\\in\\mathbb{R}$, and $\\epsilon$ is an infinitesimal unit. Using $ \\epsilon^2 = 0 $, we can define arithmetic operations for these numbers. For example,\n","$$\\xi_1\\cdot \\xi_2 = (y_1 + \\epsilon y_1')(y_2 + \\epsilon y_2') = y_1y_2 + \\epsilon(y_1y_2' + y_1'y_2).$$\n","Notice that the second term on the right hand side is suggestive of the product rule. Likewise,\n","$$\\xi^2 = \\left(x + \\epsilon x'\\right)^2 = x^2 + \\epsilon(2xx').$$\n","\n","Numerically, one can implement dual numbers in a similar way as complex numbers. We simply staple together two floating point numbers, one for each part. We can then define operators for arithmetic operations involving dual numbers."],"metadata":{"id":"tYUPpuCE8txg"}},{"cell_type":"code","source":[],"metadata":{"id":"CLIJ_tbl9JQF"},"execution_count":null,"outputs":[]}]}